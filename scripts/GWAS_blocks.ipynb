{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Make GWAS blocks\n",
    "\n",
    "This notebooks partitions GWAS signal into blocks and extract the smallest one per block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using SQL via `pandas` -- here I do it via `pandas` because I believe my computer has enough memory (64GB) for the job. But in practice one can build the input data to a database and adapt the SQL queries to running on physical databases.\n",
    "\n",
    "The parameter `--get-top` will instead only keep the top variant per block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Usage\n",
    "\n",
    "```\n",
    "sos run GWAS_blocks --filename ~/Downloads/combined_pm_MVP.rds --increase-by 500000 --cwd ~/tmp/18-Jul-2020\n",
    "```\n",
    "\n",
    "## Data\n",
    "\n",
    "The input data I received is in RDS format and looks like:\n",
    "\n",
    "```\n",
    "> head(readRDS('combined_lfsr_MVP.rds'))\n",
    "                  hdl       ldl        tg        tc\n",
    "1:100000012 0.1787311 0.4427086 0.2006725 0.5006527\n",
    "1:10000006  0.6073020 0.5758349 0.6109343 0.5880965\n",
    "1:100000507 0.4475880 0.3671566 0.4273243 0.3784221\n",
    "1:100000827 0.1137560 0.4597413 0.1258910 0.5036999\n",
    "1:100000843 0.4532421 0.5815971 0.4456185 0.5834053\n",
    "1:100000948 0.3151570 0.5017485 0.3500289 0.4714799\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('.')\n",
    "# File input is zipped and is chr:pos score_1, ...\n",
    "parameter: filename = path('~/Downloads/combined_pm_MVP.rds')\n",
    "# Increase by 0.5Mb\n",
    "parameter: increase_by = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Convert RDS file to text file\n",
    "[extract_1]\n",
    "depends: R_library('data.table')\n",
    "input: filename\n",
    "output: f'{cwd:a}/{_input:bn}.gz'\n",
    "R: expand = \"${ }\"\n",
    "    dat = readRDS(${_input:r})\n",
    "    # make rowname the first column\n",
    "    # `check.names=TRUE` will \n",
    "    data.table::fwrite(data.frame(dat, check.names=TRUE, stringsAsFactors=FALSE), ${_output:r}, row.names=TRUE, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[extract_2]\n",
    "depends: Py_Module('pandas', autoinstall=True), Py_Module('pandasql', autoinstall=True)\n",
    "parameter: get_top = False\n",
    "python: expand = \"${ }\"\n",
    "    import pandas as pd\n",
    "    from pandasql import sqldf\n",
    "    # load data\n",
    "    print(\"loading data ...\")\n",
    "    dat = pd.read_csv(${_input:r}, header=0)\n",
    "    dat.columns = ['variant_id'] + dat.columns[1:].to_list()\n",
    "    phenotypes = dat.columns.values[1:]\n",
    "    # get proper chr and pos\n",
    "    print(\"adding chr and pos ...\")\n",
    "    new_cols = dat[\"variant_id\"].str.split(\":\", n = 1, expand = True) \n",
    "    dat['chr'] = new_cols[0].astype(int)\n",
    "    dat['pos'] = new_cols[1].astype(int)\n",
    "    # Get ranges of each chromosome\n",
    "    print(\"preparing block table ...\")\n",
    "    blk = sqldf(\"SELECT dat.chr, min(dat.pos), max(dat.pos) FROM dat GROUP BY dat.chr\")\n",
    "    # Generate blocks table the same format as the table for LD blocks\n",
    "    def get_blocks(chrom, start, end):\n",
    "        res = []\n",
    "        j = 1\n",
    "        prev = start\n",
    "        for curr in range(start, end, ${increase_by}):\n",
    "            if start == curr:\n",
    "                continue\n",
    "            res.append([chrom, prev, curr, f'chr{chrom}_{j}'])\n",
    "            prev = curr\n",
    "            j += 1\n",
    "        if curr < end:\n",
    "            res.append([chrom, curr, end, f'chr{chrom}_{j}'])\n",
    "        return res\n",
    "    blk = sum([get_blocks(row[0], row[1], row[2]) for idx, row in blk.iterrows()], [])\n",
    "    blk = pd.DataFrame(blk, columns = ['chr', 'start', 'end', 'blk_id'])\n",
    "    blk.to_csv(\"${filename:n}\" + '.blk.gz', index=False)\n",
    "    # the core queries, first create a joint table with blk_id annotation, then query from there\n",
    "    print(\"joining tables ...\")\n",
    "    query = f\"SELECT dat.variant_id, dat.chr, dat.pos, {', '.join(['dat.' + y for y in phenotypes])}, blk.start, blk.end, blk.blk_id FROM dat LEFT JOIN blk WHERE dat.chr == blk.chr AND (dat.pos >= blk.start AND dat.pos <= blk.end)\"\n",
    "    dat = sqldf(query)   \n",
    "    print(\"running variant extraction queries ...\")\n",
    "    if (${get_top}):\n",
    "        queries = [f\"SELECT variant_id, chr, pos, min({y}), start, end, blk_id FROM dat GROUP BY blk_id ORDER BY chr, pos\" for y in phenotypes]\n",
    "    else:\n",
    "        queries = [f\"SELECT variant_id, chr, pos, {y}, start, end, blk_id FROM dat ORDER BY chr, pos\" for y in phenotypes]        \n",
    "    res = [sqldf(query) for query in queries]\n",
    "    # save result\n",
    "    for idx, item in enumerate(res):\n",
    "        item.drop(columns=['chr', 'pos'], inplace=True)\n",
    "        item.columns = ['variant_id', phenotypes[idx], 'bin_start', 'bin_end', 'blk_id']\n",
    "        item.to_csv(\"${cwd}/${filename:bn}\" + f'.{phenotypes[idx]}.gz', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Additional steps for input with `torus` pipeline\n",
    "\n",
    "See the workflow [here](https://github.com/cumc/bioworkflows/blob/master/fine-mapping/gwas_enrichment.ipynb).\n",
    "\n",
    "### LD blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "#fn=combined_pm_MVP\n",
    "fn=original_z_mvp\n",
    "zcat $fn.blk.gz | tail -n+2 | sed 's/^/chr/g' > $fn.blocks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### SNP map and z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "for i in hdl ldl tc tg; do\n",
    "    zcat $fn.$i.gz | tail -n+2  | awk '{print $1,$5,$2}' OFS='\\t' | gzip --best > $fn.$i.zscore.gz\n",
    "    zcat $fn.$i.zscore.gz | cut -f1 | awk -F':' '{print \"chr\"$1\".\"$2,$1,$2}' OFS='\\t' | gzip --best > $fn.$i.smap.gz\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.21.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
